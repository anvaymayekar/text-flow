# ğŸ“Š **TextFlow** â€” A 2nd-Order Markov Chain Text Generator

A probabilistic text generation engine built from scratch using **2nd-order Markov Chains** in **pure Python**, demonstrating mathematical foundations of stochastic processes, state transitions, and natural language modeling across diverse literary and technical corpora.

> ğŸ“ This project was developed as an **educational exploration** of statistical language modeling, Markov processes, and probabilistic text generation, showcasing the mathematical elegance of simple yet powerful algorithms without relying on deep learning frameworks.

---

## ğŸ“Œ Highlights & Mathematical Foundation

> ğŸ§® **2nd-Order Markov Property**: The model predicts the next character based on the previous **two characters**, creating a context-aware probability distribution.
>
> ğŸ“ˆ **Sparse Transition Matrices**: Uses dictionary-based sparse representations instead of dense matrices, enabling memory-efficient storage of transition probabilities.
>
> ğŸ² **Laplace Smoothing**: Implements additive smoothing (Î± = 1e-8) to handle unseen character sequences and prevent zero-probability catastrophes.
>
> ğŸ”„ **Atomic File Operations**: YAML state persistence and pickle serialization ensure training resumability and data integrity.
>
> ğŸ“š **Multi-Domain Corpus**: Trained on literary classics (Austen, Shakespeare, Harper Lee), technical texts (ML, robotics), and cultural content (Indian literature, languages).

---

## ğŸ§® Mathematical Framework

### Markov Chain Theory

A **second-order Markov chain** assumes that the probability of the next state depends only on the two most recent states:

```math
P(X_n \mid X_1, X_2, \ldots, X_{n-1})
=
P(X_n \mid X_{n-2}, X_{n-1})
```

### State Representation

Each state is defined as an ordered pair of consecutive characters:

```math
s_i = (c_{i-1}, c_i)
```

Transitions are defined as probabilities of the next character given the current state:

```math
P(c_{i+1} \mid c_{i-1}, c_i)
```

### Probability Calculations

#### Initial State Distribution

The initial state distribution $\pi$ is computed using additive (Laplace) smoothing:

```math
\pi(s) =
\frac{count(s) + \alpha}
{\sum_{s' \in S} count(s') + |S| \alpha}
```

#### Transition Probabilities

The transition probability from state $s_i$ to state $s_j$ is defined as:

```math
P(s_j \mid s_i) =
\frac{count(s_i \rightarrow s_j) + \alpha}
{\sum_k count(s_i \rightarrow s_k) + |S| \alpha}
```

#### Where

-   $$\alpha$$ â€” smoothing parameter (default: $$10^{-8}$$)
-   $$S$$ â€” set of all unique states
-   $$|S|$$ â€” total number of states
-   $$\text{count}(s_i \rightarrow s_j)$$ â€” observed transitions from $$s_i$$ to $$s_j$$

### Text Generation Algorithm

1. **Initialization**
   Sample the initial state $s_0$ from the distribution $\pi$.

2. **Propagation**
   For each generation step $t$:

    - Retrieve transition probabilities $P(\cdot \mid s_t)$
    - Sample the next character using weighted random selection

3. **State Update**

```math
s_{t+1} = (c_t, c_{t+1})
```

4. **Termination**
   Stop after generating $n$ characters.

### Anti-Repetition Mechanism

To reduce repetitive loops, a context-aware penalty is applied to recently generated states:

```math
P'(s) = P(s) \cdot \gamma \quad \text{if } s \in recent\_context
```

```math
P'(s) = P(s) \quad \text{otherwise}
```

where:

-   $\gamma \in [0.01, 0.1]$ is the penalty factor
-   `recent_context` denotes a sliding window of recently generated states

This mechanism lowers the probability of repeating recent patterns while preserving overall stochasticity.

---

## ğŸ“ File Structure

```
textflow/
â”œâ”€â”€ main.py                     # ğŸš€ Main entry point for text generation
â”œâ”€â”€ project.conf                # âš™ï¸ Project configuration file
â”œâ”€â”€ README.md                   # ğŸ“˜ Project documentation
â”œâ”€â”€ LICENSE                     # âš–ï¸  MIT License
â”œâ”€â”€ requirements.txt            # ğŸ“¦ Python dependencies
â”œâ”€â”€ .gitignore                  # ğŸš« Git exclusions
â”‚
â”œâ”€â”€ venv/                       # ğŸ Virtual environment (auto-created)
â”‚
â”œâ”€â”€ utils/                      # ğŸ› ï¸  Utility modules
â”‚   â”œâ”€â”€ __init__.py             # Package initialization
â”‚   â”œâ”€â”€ config.py               # âš™ï¸  Configuration file parser (.conf)
â”‚   â”œâ”€â”€ constants.py            # ğŸ”¢ Path constants from YAML
â”‚   â””â”€â”€ log.py                  # ğŸ“ Logging functionality
â”‚
â”œâ”€â”€ model/                      # ğŸ§  Training data and model state
â”‚   â”œâ”€â”€ __init__.py             # Package initialization
â”‚   â”œâ”€â”€ corpus.txt              # ğŸ“š Training text corpus (~226KB)
â”‚   â”œâ”€â”€ info.yaml               # â„¹ï¸  Model configuration & metadata
â”‚   â”œâ”€â”€ params.yaml             # ğŸ“Š Training progress tracker
â”‚   â”œâ”€â”€ trainer.py              # ğŸ‹ï¸  Markov chain trainer
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                   # ğŸ’¾ Serialized model weights
â”‚   â”‚   â”œâ”€â”€ initial.pkl         # Initial state probabilities (Ï€)
â”‚   â”‚   â””â”€â”€ transitions.pkl     # Transition probability matrix (P)
â”‚   â”‚
â”‚   â””â”€â”€ logs/                   # ğŸ“œ Training logs
â”‚       â””â”€â”€ sessions.log        # Timestamped training events
â”‚
â”œâ”€â”€ generator/                  # ğŸ² Text generation engine
â”‚   â”œâ”€â”€ __init__.py             # Package initialization
â”‚   â”œâ”€â”€ markov.py               # ğŸ”® Core Markov generator class
â”‚   â””â”€â”€ debugger.py             # ğŸ› Model diagnostics & debugging
â”‚
â””â”€â”€ samples/                    # ğŸ–¼ï¸  Demo outputs & screenshots
    â””â”€â”€ (sample images)         # Generated text examples
```

---

## âš™ï¸ Core Components

### 1. **Trainer (`model/trainer.py`)**

-   **Incremental Training**: Resumes from last checkpoint using cursor-based file streaming
-   **Chunked Processing**: Reads corpus in 64KB chunks to handle large files
-   **Word-Level Bigrams**: Processes text as word pairs `(wáµ¢â‚‹â‚, wáµ¢) â†’ wáµ¢â‚Šâ‚`
-   **Atomic State Persistence**: YAML + Pickle for crash-resistant training
-   **Progress Tracking**: tqdm integration for visual feedback

### 2. **Generator (`generator/markov.py`)**

-   **Dictionary-Based Transitions**: Sparse storage (only non-zero probabilities)
-   **Memory Optimization**: Limits to top 8000 most active states
-   **Multiple Sampling Modes**:
    -   Deterministic (argmax selection)
    -   Stochastic (weighted random sampling)
    -   Context-aware (anti-repetition penalties)
-   **Smoothing**: Laplace smoothing prevents zero-probability failures

### 3. **Configuration System**

-   **`info.yaml`**: Model hyperparameters (order=2, preprocessing flags)
-   **`params.yaml`**: Training state (cursor position, word count, line number)
-   **Atomic Updates**: Temp file + `os.replace()` for crash safety

### 4. **Logging (`utils/log.py`)**

-   **Dual Output**: Writes to both file and console
-   **YAML-Configured**: Log level, format, and file path from `info.yaml`
-   **Structured Messages**: DEBUG, INFO, WARNING, ERROR, CRITICAL levels

---

## ğŸ§ª Installation & Setup

### Prerequisites

-   Python 3.8+ (tested on 3.10)
-   pip package manager
-   Git (for cloning)

### Step-by-Step Installation

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/textflow.git
cd textflow

# 2. Create virtual environment
python -m venv venv

# 3. Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# 4. Install dependencies
pip install -r requirements.txt

# 5. Run the generator
python main.py
```

---

## ğŸ“¦ `requirements.txt`

```txt
# Core dependencies
numpy>=1.24.0,<2.0.0
pyyaml>=6.0
tqdm>=4.65.0

# Optional (for debugging)
matplotlib>=3.7.0  # Visualization of probability distributions
pandas>=2.0.0      # Data analysis utilities

# Development dependencies (optional)
pytest>=7.3.0      # Testing framework
black>=23.0.0      # Code formatting
```

---

## ğŸš€ Usage Examples

### Basic Text Generation

```python
from generator import Markov

# Initialize generator (trains if needed)
markov = Markov(max_states=8000, smoothing_alpha=1e-8)

# Generate 50 words
text = markov.generate_words(num_words=50, deterministic=False)
print(text)
```

### Advanced Options

```python
# Deterministic generation (always picks highest probability)
text_det = markov.generate_words(num_words=30, deterministic=True)

# With anti-repetition (reduces loops)
text_varied = markov.generate_words(
    num_words=100,
    anti_repetition=True,
    context_window=5
)

# Get model statistics
stats = markov.get_vocabulary_stats()
print(f"States: {stats['num_states']}, Vocab: {stats['vocabulary_size']}")
```

### Debugging & Analysis

```bash
# Run comprehensive model diagnostics
python -m generator.debugger
```

This will output:

-   Model statistics (states, vocabulary, transitions)
-   Top initial bigrams
-   Dead-end states analysis
-   Sample generations with different methods
-   Improvement suggestions

---

## ğŸ§  Model Architecture

### State Space Design

**State Definition:**

```python
State = Tuple[str, str]  # (word_i-1, word_i)
```

**Example States:**

```
("the", "quick") â†’ ["brown", "lazy", "slow"]
("quick", "brown") â†’ ["fox"]
```

### Probability Storage

**Initial Distribution (Ï€):**

```python
{
    ("it", "is"): 0.0342,
    ("the", "world"): 0.0156,
    ...
}
```

**Transition Matrix (P):**

```python
{
    ("it", "is"): {
        ("is", "a"): 0.45,
        ("is", "not"): 0.23,
        ("is", "the"): 0.18,
        ...
    },
    ...
}
```

### Training Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Read Corpus â”‚ (streaming, 64KB chunks)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokenize Words  â”‚ (lowercase, alphabetic only)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Count Bigrams    â”‚ (prev_prev, prev) â†’ current
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Update Pickles   â”‚ (atomic write with .tmp)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save Checkpoint  â”‚ (cursor, line, count in YAML)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Experimental Features

### 1. **Adaptive Smoothing**

Dynamically adjusts Î± based on corpus size:

```python
Î± = max(1e-8, 1.0 / sqrt(vocab_size))
```

### 2. **Context Tracking**

Maintains sliding window of recent states to avoid repetition loops.

### 3. **State Pruning**

Limits model to top-N most frequent states (default: 8000) to prevent memory explosion.

### 4. **Probability Validation**

Ensures all transition rows sum to 1.0 (within numerical tolerance).

---

## ğŸ“Š Performance Metrics

| Metric           | Value                      |
| ---------------- | -------------------------- |
| Corpus Size      | ~226 KB (raw text)         |
| Vocabulary       | ~35,000 unique words       |
| States           | ~8,000 (after pruning)     |
| Transitions      | ~150,000 (sparse)          |
| Training Time    | ~2-5 seconds (incremental) |
| Generation Speed | ~1000 words/sec            |
| Memory Usage     | ~50 MB (loaded model)      |

---

## ğŸ¨ Sample Outputs

<p align="center">
  <img src="sample/001.jpg" alt="text-flow output demo" width="800"/>
</p>

---

## ğŸ› Debugging Tools

### `debugger.py` Output Structure

```
============================================================
MODEL STATISTICS
============================================================
num_states               : 8000
vocabulary_size          : 34775
initial_states           : 7845
...

============================================================
RAW DATA ANALYSIS
============================================================
Initial bigram states: 7845
Top 10 initial bigrams:
  1. (it          , is          ) ->  342 times
  2. (the         , world       ) ->  156 times
  ...

============================================================
IMPROVEMENT SUGGESTIONS
============================================================
âš ï¸  Very sparse transitions - model may produce repetitive text
   Try: Increase smoothing_alpha or add more training data
âœ…  Contains 'Pride and Prejudice' vocabulary
```

---

## ğŸ”§ Configuration Files

### `info.yaml`

```yaml
model:
    type: "MarkovChain"
    level: "character" # or "word"
    order: 2
    preprocess:
        alphabetic_only: true
        lowercase: true
        remove_punctuation: true
        space_normalization: true

paths:
    info: "model/info.yaml"
    corpus: "model/corpus.txt"
    params: "model/params.yaml"
    initial: "model/data/initial.pkl"
    transitions: "model/data/transitions.pkl"
    logs: "model/logs/sessions.log"

logging:
    level: INFO
    format: "%(asctime)s [%(levelname)s] %(message)s"
```

### `params.yaml` (Auto-Updated)

```yaml
corpus:
    cursor: 225801 # Byte position in corpus
    count: 34775 # Total words processed
    line: 2777 # Lines processed
```

---

## ğŸš§ Known Limitations

1. **Repetition Loops**: Despite anti-repetition measures, the model can occasionally fall into cycles (inherent to low-order Markov chains)
2. **Grammar Awareness**: No syntactic understandingâ€”output may be grammatically incorrect
3. **Context Window**: Limited to 2 characters (increasing order exponentially increases memory)
4. **Sparse Data**: Rare character combinations may generate unlikely sequences
5. **No Semantic Understanding**: Purely statistical; doesn't "understand" meaning

---

## ğŸ”® Future Enhancements

-   [ ] **Higher-Order Models**: Experiment with 3rd/4th-order Markov chains
-   [ ] **Hybrid Approach**: Combine character and word-level models
-   [ ] **Temperature Sampling**: Add temperature parameter for creativity control
-   [ ] **Beam Search**: Implement beam search for more coherent generation
-   [ ] **Corpus Expansion**: Add more diverse training data
-   [ ] **Interactive Mode**: Live web interface for text generation
-   [ ] **Model Comparison**: Benchmark against GPT-2/LSTM baselines

---

## ğŸ“š Educational Value

This project demonstrates:

âœ… **Stochastic Processes**: Practical application of Markov chains  
âœ… **Probability Theory**: Conditional probabilities, smoothing, normalization  
âœ… **Algorithm Design**: State space management, sparse storage, incremental training  
âœ… **Software Engineering**: Modular design, logging, configuration management, atomic operations  
âœ… **Data Structures**: Hash maps, sparse matrices, probability distributions

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

-   Additional corpus sources (more languages, domains)
-   Alternative smoothing techniques (Kneser-Ney, Good-Turing)
-   Visualization tools (state transition graphs, probability heatmaps)
-   Performance optimizations (Cython, multiprocessing)
-   Unit tests and benchmarks

---

## âš–ï¸ License

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT).  
You are free to use, modify, and distribute this software with proper attribution.

---

## ğŸ‘¨â€ğŸ’» Author

> **Anvay Mayekar**  
> ğŸ“ B.Tech in Electronics & Computer Science â€” SAKEC, Mumbai
>
> [![GitHub](https://img.shields.io/badge/GitHub-181717.svg?style=for-the-badge&logo=GitHub&logoColor=white)](https://www.github.com/anvaymayekar) > [![LinkedIn](https://img.shields.io/badge/LinkedIn-0A66C2.svg?style=for-the-badge&logo=LinkedIn&logoColor=white)](https://in.linkedin.com/in/anvaymayekar) > [![Gmail](https://img.shields.io/badge/Gmail-D14836.svg?style=for-the-badge&logo=gmail&logoColor=white)](mailto:anvaay@gmail.com)
